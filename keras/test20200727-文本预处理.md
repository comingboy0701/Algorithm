```python
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import numpy as np
import pandas as pd
import jieba 
from gensim.models import Word2Vec
from keras.utils import to_categorical
import tensorflow as tf
from keras import metrics
```

```python
train_path = '/Users/coming/darcytech/project05_bert/datasets/lcqmc/train.tsv'
valid_path = '/Users/coming/darcytech/project05_bert/datasets/lcqmc/dev.tsv'
test_path = '/Users/coming/darcytech/project05_bert/datasets/lcqmc/test.tsv'

train_data = pd.read_csv(train_path,sep='\t',header=None, names =['text1','text2','label'],skiprows=1)
train_data['mark'] = 1

valid_data = pd.read_csv(valid_path,sep='\t',header=None, names =['text1','text2','label'],skiprows=1)
valid_data['mark'] = 0

test_data = pd.read_csv(test_path,sep='\t',header=None, names =['text1','text2','label'],skiprows=1)
test_data['mark'] = -1

data = pd.concat([train_data,valid_data,test_data],axis=0)
```

```python
data['text1'] = data[['text1']].applymap(lambda x:list(jieba.cut(x)))
data['text2'] = data[['text2']].applymap(lambda x:list(jieba.cut(x)))
```

```python
data.head()
```

```python
def train_save_word2vec(docs, embed_size=100, save_name='./data/word.vec'):
    w2v = Word2Vec(docs, size=embed_size, sg=1, window=8, seed=2018, workers=4, min_count=5, iter=3)
    w2v.wv.save_word2vec_format(save_name)
    return w2v
```

```python
docs = list(data['text1'])+list(data['text2'])
embed_size = 100
maxlen = 80
w2v = train_save_word2vec(docs,embed_size=embed_size)
```

```python
tokenizer = Tokenizer(num_words=None, 
                       filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n', 
                       lower=False, 
                       split='', 
                       char_level=True, 
                       oov_token=None, 
                       document_count=0)
tokenizer.fit_on_texts(docs)
word_index=tokenizer.word_index
```

```python
nb_words = len(word_index)+1
emb = np.zeros((nb_words, embed_size))
count = 0
for word, i in word_index.items():
    if i >= nb_words:
        continue
    try:
        embedding_vector = w2v[word]
    except:
        embedding_vector = np.zeros(embed_size)
        count += 1
    if embedding_vector is not None:
        emb[i] = embedding_vector
```

```python
docs =[ i+['|']+j for i ,j in zip(list(data['text1']),list(data['text2']))]
X = tokenizer.texts_to_sequences(docs)
X = pad_sequences(X, maxlen=maxlen, value=0,truncating='post',padding='post')
```

```python
train_x ,train_y = X[data['mark']==1,:],data.query('mark==1')[['label']].values
train_y = to_categorical(train_y)
valid_x ,valid_y = X[data['mark']==0,:],data.query('mark==0')[['label']].values
valid_y = to_categorical(valid_y)
test_x ,test_y = X[data['mark']==-1,:],data.query('mark==-1')[['label']].values
test_y = to_categorical(test_y)
```

```python
from keras.layers import Input,Embedding,LSTM,Dense
from keras.models import Model
from keras import backend as K
from keras.utils import plot_model

nb_classes = 2
encode_size=128

input_x = Input(shape=(None,))
embedded = Embedding(emb.shape[0],
                     output_dim=emb.shape[1],
                     weights=[emb],
                     input_length=maxlen,
                     trainable=True)(input_x)

encoder = LSTM(encode_size)(embedded)
predict = Dense(nb_classes, activation='softmax')(encoder)

@tf.function
def my_crossentropy(y_true, y_pred, e=0.1):
    loss1 = K.categorical_crossentropy(y_true, y_pred)
    loss2 = K.categorical_crossentropy(K.ones_like(y_pred)/nb_classes, y_pred)
    return (1-e)*loss1 + e*loss2

@tf.function
def my_metric(y_true, y_pred):
    return K.mean(K.cast(K.argmax(y_true,axis=1)==K.argmax(y_pred,axis=1),dtype='float32'))
```

```python
model = Model(inputs=input_x, outputs=predict)
model.compile(optimizer='adam', loss=my_crossentropy, metrics=[my_metric])
```

```python
model.fit(train_x,train_y,validation_data=[valid_x, valid_y],epochs=3,batch_size=128)
```

```python
model.evaluate(valid_x, valid_y)
```

```python
model.evaluate(test_x, test_y)
```

```python

```
